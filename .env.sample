# LLM Provider API Keys
# Replace with your actual API keys
GEMINI_API_KEY="your-gemini-api-key-here"
OPENAI_API_KEY="your-openai-api-key-here"
ANTHROPIC_API_KEY="your-anthropic-api-key-here"
OPENROUTER_API_KEY="your-openrouter-api-key-here"

# Primary Model Selection (Refer to llm_config.json for defaults if not set)
# These variables allow you to override the default model for each provider.
# Example: GEMINI_MODEL_PRIMARY="gemini-1.5-pro-latest"
GEMINI_MODEL_PRIMARY=""
OPENAI_MODEL_PRIMARY=""
ANTHROPIC_MODEL_PRIMARY=""
OPENROUTER_MODEL_PRIMARY=""

# Google Cloud project settings (Optional - for Vertex AI Gemini client)
# GEMINI_PROJECT_ID="your-google-cloud-project-id"
# GEMINI_LOCATION="your-google-cloud-region" # e.g., us-central1

# Logging Configuration
# LOG_DIR="custom_logs" # Default is "logs"

# Caching Configuration
# LLM_CACHE_ENABLED="false" # Default is true. Set to "false" to disable.

# GitHub Token (if needed for other utilities)
GITHUB_TOKEN="your-github-token-here"
